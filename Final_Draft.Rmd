---
title: "Predicting Illness Visits With Neural Networks"
author: "David Skrill"
date: "12/15/2020"
output:
  pdf_document: 
    toc: true
    toc_depth: 3
    latex_engine: pdflatex
    pandoc_args: "--pdf-engine-opt=--shell-escape"
    extra_dependencies: ["float"]
urlcolor: blue
  
header-includes: 
  - \usepackage{svg}
  - \usepackage{longtable}
  - \usepackage{float}
  - \floatplacement{figure}{H}


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,message = F,warning = F,dev = 'pdf',fig.pos = '!H',cache = T)
```

```{r}
rm(list=ls())
library(tidyverse)
library(janitor)
library(knitr)
library(kableExtra)
library(gridExtra)
```


# Introduction

The data considered here are taken from a recently completed study at University of Rochester. The data consists of clinical and molecular measurements and questionaire responses for 166 infants recruited at birth followed for up to three years. The primary goal of this project is to predict whether or not an infant had a respiratory illness at any particular visit, as self-reported by the infant's parent. Modelling was performed using the Least Absolute Shrinkage and Selection Operator (LASSO) and neural networks. The paper preceeds as follows: an introduction to the data and outline of the preprocessing steps taken; presentation and discussion of LASSO results; presentation and discussion of various neural network architectures and results.

\newpage 

# Data Preprocessing

The full data consists of the following information:

|Description           |  Observations| Features|
|:---------------|-----:|--------:|
|Birth medical history            |   166|       21|
|Birth Demographics            |   166|        5|
|Family demographics             |   166|        2
|oxygen exposure at birth             |   166|        3|
|Pregnancy  medical history            |   166|       47|
|immune profiling   |   278|       83|
|Follow up survey 1             |   635|        7|
|Follow up survey 2             |   316|       32|
|**Target variables** and time series info |  2339|        5|
|Nasal microbiome  |  1242|      474|
|Rectal microbiome  |  1481|      490|
|Throat microbiome  |   334|      128|
|Virus and bacteria testing            | 42419|        6|
|vaccination record        |  2943|        4|

Subjects are uniquely identified by Alias and a particular visit for an infant is uniquely keyed by the triplet $(\textit{Alias, Visit ID, pCGA})$, where the Visit ID is a sequential count of previous visits for an infant and pCGA is quantized corrected gestational age. Prior to any processing, the data was split by Alias into training, validation 1, validation 2, and testing data, following a 60%, 15%, 15%, 10% split, respectively. 

Selected preprocessing steps are detailed below; for a full account of the preprocessing, see Appendix. 

## Microbiome and Immune Profiling Data

Each of the microbiome data and the flowcytometry data were filtered to remove all-zero columns and retain distinct Alias, Visit ID, pCGA keys. Missing values were present in the flowcytometry data; these were imputed by Alias using Last Observation Carried Forward (LOCF); any remaining missing values were mean-imputed using column means. Principal component analysis was performed on the resulting tables, producing the following screeplots:

```{r,fig.cap="PCA of Microbiome and Flowcytometry Data"}
nas <- readRDS("training_objects/nas_PCA.rds")
rec <- readRDS("training_objects/rec_PCA.rds")
thr <- readRDS("training_objects/thr_PCA.rds")
flow <- readRDS("training_objects/flow_PCA.rds")

get_scree <- function(pca_obj,title){
  var_explained_df = data.frame(PC= 1:length(pca_obj$sdev),
                               var_explained=(pca_obj$sdev)^2/sum((pca_obj$sdev)^2))
  output = var_explained_df %>% ggplot(aes(x=PC,y=var_explained))+
    geom_point(size=1)+
    geom_line()+
    ylab(label = "Variation Explained")+
    labs(title=title)
  return(output)
}

NAS <- get_scree(nas,"Nasal Microbiome")
THR <- get_scree(thr,"Throat  Microbiome")
REC <- get_scree(rec,"Rectal  Microbiome")
FLOW <- get_scree(flow,"Flowcytometry")
grid.arrange(NAS,THR,REC,FLOW,ncol=2)
```

The first 50 principal components were retained for the microbiome data; 20 were retained for the flowcytometry data. 

## Other Proprocessing

One-visit lags were created by Alias for the microbiome, flowcytometry, and virus and bacteria testing data (the longitudinal data). All-missing variables were dropped, and the least-frequent levels for categorical variables were lumped  together into an "Other" category, ensuring that "Other" is the smallest category. All data were joined by Alias, Visit ID, and pCGA in cases where such a join was possible; otherwise, the join was done by Alias and pCGA or Alias and Visit ID. Missing values in the resulting table were imputed by Alias, first by LOCB, then LOCF. Any remaining missing values were imputed using the column-wise mean value. 


## Summary Statistics

```{r,results='asis',fig.cap="Summary statistics of the training data for variables ultimately used in modeling (as chosen by logistic LASSO)"}
source("Rfunctions/sally/summarize.R")
lasso_selected <- readRDS("lasso/lasso_selected")
lasso_selected <- lasso_selected$X1
training_data <- read_csv("training/training.csv")
training_data <- readRDS("design_matrices/X_training")

long_names <- c("Birth Weight (g)", "TPiece Resuscitator","Cohort 26-27 weeks","Mother's Edu., Other", 
                "Placental Pathology Obtained", "Visit ID", "PC1 Flow","PC3 Flow","PC4 Flow","PC7 Flow",
                "PC14 Flow","PC18 Flow","PC1 Flow lag","PC5 Flow lag", "How many smokers in home?", "PC3 Nasal",
                "PC16 Nasal","PC3 Nasal lag","PC5 Nasal lag","PC15 Nasal lag","PC31 Nasal lag", "PC39 Nasal log",
                "PC45 Nasal lag","PC49 Nasal lag","PC1 Rectal","PC41 Rectal","PC43 Rectal",
                "PC44 Rectal","PC50 Rectal", "PC7 Rectal lag", "PIV2","RSV","Rhino","MHom","BPert","PIV3","HMPV","Corona 3","MPneu","Parecho",
                "Corona 2","RSV lag", "MHom lag","PIC 3 lag","MPneu lag", "SPneu lag")

temp = training_data %>% 
  as_tibble() %>% 
  select(all_of(lasso_selected)) %>% 
  #select(-contains("pc")) %>% 
  `names<-`(long_names) %>% 
  as.data.frame()
summary_stats = summarize(temp,uniq=T,latex = T)
```

\newpage

# Modeling: LASSO

The first model considered is the logistic LASSO. Penalty parameter values ranged from $\log(-12)$ to $\log(-3)$; the selected parameter, $\log(-4.48)$, maximized the area under the ROC curve produced using predictions on validation 1 data.  

## Solution Path

```{r}
library(glmnet)
X_training <- readRDS("design_matrices/X_training")
y_training <- readRDS("design_matrices/y_training")

X_validation1 <- readRDS("design_matrices/X_validation1")
y_validation1 <- readRDS("design_matrices/y_validation1")


X_testing <- readRDS("design_matrices/X_testing")
y_testing <- readRDS("design_matrices/y_testing")
lasso <- glmnet(x=X_training,y=y_training,family="binomial",
                        parallel = T,
                        relax = F,
                        trace.it = F,
                        path=T,
                        standardize = T,
                        keep = T,
                        maxit = 10^5,
                alpha=1)
best_lambda <- readRDS("lasso/best_lambda")
```

```{r, fig.cap="LASSO solution path"}
plot(lasso,"lambda")
abline(v=log(best_lambda),lty=2)
```

It is notable that the selected $\lambda$ is relatively large, and hence sets many coefficients to 0. At the selected value, there remain 46 coefficients in the model. It is also interesting to note that a single coefficient is extremely large at lower values of $\lambda$; it is my suspicion that the training data may be nearly separable along this variable. However, that a higher value of $\lambda$ was ultimately chosen, thus constraining this coefficient to a more reasonable range suggests that the validation data is probably not seperable in the same way. 

## Selected Coefficients


The coefficient on PC Nasal lag is unexpectedly large in magnitude; Investigation reveals that one Alias had very low values of this variable during a stretch of illness visits. The selected coefficients confirm what might be expected: that positive or higher bacteria and virus testing results are associated, for the most part, with greater odds of an illness. That so many higher principal components and principal component lags were selected is somewhat surprising, and may suggest that a greater number of principal components be considered. However, given the very small amount of variation explained by any additional principal components, I suspect that doing so would not lead to the detection of generalizable effects.  

```{r, fig.cap="Non-zero LASSO Coefficient Values at penalty paramter value log(-4.48)"}
coef_table <- broom::tidy(lasso) %>% filter(lambda==best_lambda) %>% select(term, estimate)
names(coef_table) = c("Coefficient","Estimate")
coef_table$Coefficient = c("Intercept",long_names)
#coef_table %>% kable(booktabs=T)


row.names(coef_table)<-NULL
rows <- seq_len(nrow(coef_table) %/% 2)
kable(list(coef_table[rows,1:2],
           matrix(numeric(), nrow=0, ncol=1),
           coef_table[-rows, 1:2]),
      caption = "Selected Coefficients",
      label = "tables", format = "latex", booktabs = F, longtable=F) 
```


## Evaluation

### Loss and AUC Curves

The penalty parameter was chosen to maximize the AUC on validation data. It seems possible that AUC is too coarse a measure to be used for model selection. However, as the following figure shows, the parameter than maximizes the validation AUC is near the minimizer of the log loss on validation data (and testing data as well), lending justification to this choice. As an additional consideration, since the LASSO will also serve as a variable selection method on inputs to neural networks in the next section, it may be desirable to favor a greater number of predictors.

```{r,fig.cap="Log-loss and AUC at varying values of lambda. The value that maximizes the validation AUC is represented by the dashed line."}

#################
# Loss curves
#################
library(ROCR)
library(reshape2)
library(tidyverse)
library(gridExtra)
colors <- c('red', 'blue','green')


X_vec <- list(X_training,X_validation1,X_testing)
y_vec <- list(y_training,y_validation1,y_testing)
best_lambda <- readRDS("lasso/best_lambda")


LogLoss <- function(actual, predicted, eps=0.00001) {
  predicted <- pmin(pmax(predicted, eps), 1-eps)
  -1/length(actual)*(sum(actual*log(predicted)+(1-actual)*log(1-predicted)))
}
loss_list = list()
for(tt in 1:3){
  output = rep(0,length(lasso$lambda))
  for(lambda in 1:length(lasso$lambda) ){
    test = predict(lasso, newx = X_vec[[tt]], s = lasso$lambda[lambda], type="response")
    actual = y_vec[[tt]]
    output[lambda] = (LogLoss(actual,test))
    
  }
  
  loss_list[[tt]] = output
}
loss_df <- loss_list %>% as.data.frame()
loss_df$lambda = lasso$lambda
names(loss_df) <- c("Training","Validation","Testing","Lambda")

dd = melt(loss_df,id="Lambda")
loss_plot = ggplot(dd) + 
  geom_line(aes(x = Lambda, y = value,colour = variable))+
  scale_colour_manual(values=colors)+
  geom_vline(xintercept =best_lambda,linetype="dashed")+
  xlab('Lambda') +
  ylab('Log-Loss')+
  labs(title="LASSO Log-Loss")

######################
# AUC Curves
########################
lvec <- lasso$lambda             
# validation_auc = rep(0,length(lvec))
# for(ll in 1:length(lvec)){
#   test = predict(lasso, newx = X_validation,s=lvec[ll], type="response")
#   pred_ROCR = prediction(test,y_validation %>% as.numeric)
#   validation_auc[ll]= performance(pred_ROCR,"auc")@y.values[[1]]
# }
# plot(lvec,validation_auc,type="l")

auc_list = list()
for(tt in 1:3){
  auc_vec = rep(0,length(lvec))
  for(ll in 1:length(lvec)){
    test = predict(lasso, newx = X_vec[[tt]],s=lvec[[ll]], type="response")
    pred_ROCR = prediction(test,y_vec[[tt]] %>% as.numeric)
    auc_vec[ll]= performance(pred_ROCR,"auc")@y.values[[1]]
  }
  auc_list[[tt]] = auc_vec
}
auc_df <- as.data.frame(auc_list)
auc_df$Lambda = lvec
names(auc_df) <- c("Training","Validation","Testing","Lambda")
dd = melt(auc_df,id="Lambda")
auc_plot <- ggplot(dd) +
  geom_line(aes(x = Lambda,y=value,colour=variable)) +
  scale_colour_manual(values=colors)+
  geom_vline(xintercept =best_lambda,linetype="dashed")+
  xlab('Lambda') +
  ylab('AUC')+
  labs(title="LASSO AUC")

grid.arrange(loss_plot, auc_plot, ncol=2)
```


### ROC Curves

```{r, fig.cap="ROC Curves for the logistic LASSO model. The AUCs for training, validation, and testing data are 0.926, 0.901, and 0.838, respectively."}




X_vec <- list(X_training,X_validation1,X_testing)
y_vec <- list(y_training,y_validation1,y_testing)

roc_list = list()
cutoff_list = list()

colors <- c('red', 'blue','green')
for(tt in 1:3){
  test = predict(lasso, newx = X_vec[[tt]],s=best_lambda, type="response")
  pred_ROCR <- prediction(test,y_vec[[tt]])
  perf = performance(pred_ROCR,"tpr","fpr")
  #print(performance(pred_ROCR,"auc")@y.values[1])
  plot(perf,add=(tt!=1),col=colors[tt],lwd=2, main="LASSO ROC Curves")
  #roc_list[[tt]] = perf@y.values
  #cutoff_list[[tt]] = perf@alpha.values
}

# names(roc_list) <- c("Training","Validation","Testing")
# dd <- melt(roc_list)
# dd$Cutoff <- unlist(cutoff_list)
# p = ggplot(dd) + 
#   geom_line(aes(x = Cutoff, y = 1-value,colour = L1))+
#   scale_colour_manual(values=colors)+
#   xlab('Lambda') +
#   ylab('Log Loss')
# 
# print(p)
legend(.7,.4,c("Training","Validation","Testing"),col = colors,lty=1)
```


# Modeling: Neural Networks

The variables selected by the LASSO were used as inputs to a series of neural networks. The architectures considered were constrained to three hidden layers; 836 architectures were sampled randomly from the following possibilities and assessed on validation 2 data:

- Hidden layer 1 # Nodes: 20, 40, 60
- Hidden layer 2 # Nodes: 20, 40, 60
- Hidden layer 3 # Nodes: 20, 40, 60
- Hidden layer 1 $\ell_2$ $\lambda$: 0, 0.01
- Hidden layer 2 $\ell_2$ $\lambda$: 0, 0.01
- Hidden layer 3 $\ell_2$ $\lambda$: 0, 0.01
- Dropout 1 Rate: 0, 0.2
- Dropout 2 Rate: 0, 0.2
- Dropout 3 Rate: 0, 0.2
- Activation Functions: ReLU, Leaky ReLU, ELU
- SMOTE: Yes, No

In cases where SMOTE was set to yes, the Synthetic Minority Oversampling Technique was used to augment the data. The SMOTE implementation came from the R library DMwR and parameters were set so that for each instance of an illness visit (the minority class) in the original data, 2 synthetic minority class instances were created; the majority class was sampled at a rate of twice the number of synthetic observations created. The resulting augmented data has a roughly balanced number of illness an non-illness visits.

A chosen activation function was used uniformly throughout, e.g. if ReLU was chosen, it was applied at each hidden layer. The sigmoid activation function was used for the output layer. All networks used a batch size of 128 and were optimized using Adam with an initial learning rate of 0.001. The learning rate was annealed by a rate of 0.2 upon plateau of the validation loss (defined as 10 epochs without improvement). All models were trained for 250 epochs. 

## Analysis of Network Architectures

A linear model was used to assess the effects of model architecture on validation AUC. All second-order interactions were considered, and the result was processed using AIC-based stepwise selection.

```{r,fig.cap = "Linear model estimates relating model architecture to validation AUC",fig.align='center'}
library(keras)
library(tensorflow)
library(tfruns)
library(scales)
all_runs <- ls_runs(runs_dir = "runs",order="metric_val_AUC",decreasing = T) %>% 
  select(metric_val_AUC,contains("flag"))
all_runs <- all_runs %>% filter(!is.na(metric_val_AUC))
names(all_runs) <- c("AUC",str_sub(names(all_runs)[-1],6))
full_model<- lm(AUC~.^2, data=all_runs)
selected_model <- step(full_model,trace = 0)
selected_model %>% broom::tidy() %>% 
  mutate(
    p.value = scales::pvalue(p.value),
    estimate = round(estimate,4)) %>%
  kable(
    col.names = c("Coefficient", "Estimate", "SE", "t", "p"),
    digits = c(0, 4, 4, 2, 3),
    align = c("l", "r", "r", "r", "r")
  )
```

It is interesting, if not particularly surprising that the coefficients on the interaction between SMOTE and regularization/dropout parameters are positive: in the augmented data scenario, it is more important to have these in the architecture to combat over-fitting. 

## Assessing a Sample Neural Network

Although the above indicates that there are statistically significant effects associated with different model architectures, it is my belief that the top fraction of models are practically interchangable. For example, for the top 100 models (as judged by validation AUC), AUC values ranged from 0.8182 to 0.8477; however, when the top model was retrained, its validation AUC was near the mean validation AUC of the top 100 model group. I believe that much of the variation between these top models results from stochastic optimization, and so little preference should be given to any particular model amoung the top group. As such, the sample model presented here is the model among the top 100 models that is predicted to produce the highest AUC by the model described in the previous section. It has the following architecture:

- Hidden layer 1 # Nodes: 40
- Hidden layer 2 # Nodes: 60
- Hidden layer 3 # Nodes: 20
- Hidden layer 1 $\ell_2$ $\lambda$: 0
- Hidden layer 2 $\ell_2$ $\lambda$: 0.01
- Hidden layer 3 $\ell_2$ $\lambda$: 0
- Dropout 1 Rate: 0
- Dropout 2 Rate: 0
- Dropout 3 Rate: 0
- Activation Function: ReLU
- SMOTE: No

\newpage 

### Loss and AUC Curves

As shown in the figure below, substantial and lasting improvement in validation AUC came only after a precipitous drop in training loss and spike in validation loss. This was very common throughout all of the top models. It is interesting that the region of greatest validation AUC improvement comes when training loss is already near its minimum value. There is little improvement after roughly 100 epochs; I suspect that the optimization has found a local minimum, and is prone to doing so due to the annealing of the learning rate.  
```{r, fig.cap="Sample neural network learning and performance."}
history <- readRDS("history3")
history <- as.data.frame(history)
history$data <- ifelse(history$data=="training","Training","Validation")
nn_loss_plot <- history %>% filter(metric=="loss") %>% ggplot() + 
  geom_line(aes(x=epoch,y=value,colour=data))+
  xlab('Epoch') +
  ylab('Loss')+
  labs(title="Neural Network Loss")

nn_auc_plot <- history %>% filter(metric=="AUC") %>% ggplot() + 
  geom_line(aes(x=epoch,y=value,colour=data))+
  xlab('Epoch') +
  ylab('AUC')+
  labs(title="Neural Network AUC")

nn_lr_plot <- history %>% filter(metric=="lr") %>% ggplot() + 
  geom_line(aes(x=epoch,y=value,colour=data))+
  xlab('Epoch') +
  ylab('Loss')+
  labs(title="Neural Network Learning Rate")

grid.arrange(nn_loss_plot, nn_auc_plot, ncol=2,heights=c(12))
```

### ROC Curves

```{r,fig.cap="ROC Curves for a sample neural network. The AUCs for training, validation, and testing data are 0.916, 0.802, and 0.843, respectively."}
M = load_model_tf("current_model3",compile=F)
M %>% compile(loss = loss_binary_crossentropy,
  optimizer =  optimizer_adam(),
  metrics = c(tf$keras$metrics$AUC(name="AUC")))

library(ROCR)
lasso_selected <- readRDS("lasso/lasso_selected")

X_testing <- readRDS("design_matrices/X_testing")
y_testing <- readRDS("design_matrices/y_testing")
X_testing_selected <- X_testing[,lasso_selected$X1]


test=evaluate(M,x=X_testing_selected,y=y_testing)
lasso_selected <- lasso_selected$X1

X_training <- readRDS("design_matrices/X_training")
X_training_selected <- X_training[,lasso_selected]
y_training <- readRDS("design_matrices/y_training")

X_validation2 <- readRDS("design_matrices/X_validation2")
X_validation2_selected <- X_validation2[,lasso_selected]
y_validation2 <- readRDS("design_matrices/y_validation2")

X_testing <- readRDS("design_matrices/X_testing")
X_testing_selected <- X_testing[,lasso_selected]
y_testing <- readRDS("design_matrices/y_testing")

X_vec <- list(X_training_selected,X_validation2_selected,X_testing_selected)
y_vec <- list(y_training,y_validation2,y_testing)

colors <- c('red', 'blue','green')
for(tt in 1:3){
  test = predict(M, x = X_vec[[tt]])
  pred_ROCR <- prediction(test,y_vec[[tt]])
  perf = performance(pred_ROCR,"tpr","fpr")
  #print(performance(pred_ROCR,"auc")@y.values[1])
  plot(perf,add=(tt!=1),col=colors[tt],lwd=2)
}
legend(.7,.4,c("Training","Validation","Testing"),col = colors,lty=1)
#![Neural Network ROC Curves](nn_roc_curves.jpeg)

```

```{r}
#top_run <- ls_runs(runs_dir = "runs",order="metric_val_AUC") %>% slice(1)
#"runs/2020-12-11T09-20-57Z"
#tensorboard --logdir runs/2020-12-11T09-20-57Z
```



# Appendix

Initial preprocessing was performed table-by-table, as described below. These preprocessed tables were then joined, and missing values were imputed using a combination of LOCB, LOCF, and mean-inputation.

####  Base

Too few births occurred outside of the study center to be of use as a predictor; this variable was dropped from the table. Birth Order is NA when the birth was not a multiple birth; NAs were set to 1. All babies that received a stabilization procedure received supplemental oxygen; supplemental oxygen was dropped from the table. For other stabilization procedures, NAs were set to "No", so that the resulting variable is binary ("Yes" or "No").

#### Preg

There were many variables that were all NA; these variables were dropped from the table. 

#### Fup1

There were many NAs in variables `Is baby receiving breast milk exclusively` and `For how many months did baby receive breast milk for more than half of feedings`, and many participants were completely unobserved. Even with imputation, these variables would be problematic; they were dropped from the table. 

This table does not contain Visit ID information, posing a potential problem in future joins with tables keyed by Alias and Visit ID; rows with distinct Alias-pCGA pairs were kept and the remainder discarded. Additionally, rows containing NAs were dropped.

#### Fup2 

An initial screen of variables was performed and variables with fewer than 10 NAs were retained.  The variable `How many smokers in home` was set to 0 if the participant reported that smoking is not allowed inside the home at all.  Missing values in `Dogs, cats, and other furry animals` and `Kerosene heater, wood burning stove, or fireplace` were set to "No" so that the resulting variables are binary.  Again, rows with NAs were then dropped, and and only a distinct Alias-pCGA pairs were retained.

#### Microbiome Data

Each microbiome table was preprocessed in the following steps:
all-zero columns were removed, and rows were filtered to retain distinct Alias-Visit ID-pCGA keys; PCA was performed on the subset of columns with names containing "k__Bacteria"; the first 50 principal components were selected.

```{r}
rm(list=ls())
library(tidyverse)
library(janitor)

#setwd("C:\\Users\\david\\Documents\\High Dimensional Data Analysis\\project")
files = tibble(files = list.files('train_360_DOL/', pattern = '*.tsv', full.names = TRUE)) %>% 
  mutate(names = str_match(files, '/([a-z12_]+)')[,2])

files = files %>% rowwise() %>% 
  mutate(data = list(read_tsv(files)), nrow = nrow(data), nfeature = ncol(data))

get_tbl = function(name) filter(files, names == name) %>% .$data %>% .[[1]]

auto_join <- function(by,dfs,join=full_join){
  Reduce(function(...){
    df1 = list(...)[[1]]
    df2 = list(...)[[2]]
    xxx = join(..., by = by)
    return(xxx)
  },dfs)
}

find_na <- function(df){
  sapply(df, function(x) sum(is.na(x)))
}

find_nonzeros <- function(df){
  sapply(df, function(x) sum(x==0)) != nrow(df)
}

filter_zero_cols <- function(df){
  return(df %>% select(-which(find_nonzeros(df)==F)))
}

for( n in files$names){assign(n, get_tbl(n))} # read in all tables and name

microbiome_plots <-function(df,title){
  temp = df %>% filter_zero_cols() %>% 
    distinct_at(.vars=c("Alias","visit_id","pCGA"),.keep_all = T)
  X = temp %>% select(contains("k__Bacteria"))
  pr = prcomp(X)
  screeplot(pr, npcs = 100,
          type = "lines",main=title)
}
```


A set of lag variables was created by Alias; the values at the first record for each Alias were set to zero. 

#### Flowcytometry

All-zero columns were discarded, as were rows with no Visit ID information. Again, rows were filtered to retain distinct Alias-Visit ID-pCGA keys. Missing values were imputed using LOCF by Alias; any remaining missing values were imputed using mean-imputation over the full table. PCA was performed on the subset of columns with names containing "Meta.Cluster"; the first 20 principal components were selected.
A set of lag variables was created by Alias; the values at the first record for each Alias were set to zero. 

#### TLDA

A set of lag variables was created by Alias; the values at the first record for each Alias were set to zero. 

#### Vaccines

An additional variable, `vaccinated` was created and set to 1 for all participants in the table. Missing values were imputed using LOCF by Alias, and any remaining NAs were set to 0. The result was filtered to retain distinct Alias-pCGA keys.

#### Joins

All tables were joined by Alias, Visit ID, and pCGA in cases where such a join was possible; otherwise, the join was done by Alias and pCGA or Alias and Visit ID. Missing values in the resulting table were imputed by Alias, first by LOCB, then LOCF. Any remaining missing values were imputed using the column-wise mean value. 

#### Additional Preprocessing

All character variables except Alias, Visit ID, and pCGA were set to factors levels were combined using `fct_lump_lowfreq()`; variables that are binary (both those that were originally binary and those that are effectively binary after the combination of levels) were encoded as 0-1; variable names were cleaned using janitor::clean_names().

