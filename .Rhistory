output
mean(output$g.improved)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,warning=F,message=F)
library(tidyverse)
# Chunk 2
set.seed(11235)
library(fda)
t.vec = seq(.1,9.9,.1)
x.basis <- create.bspline.basis(range(t.vec),nbasis = round(.8*length(t.vec)))
inprod.mat <- inprod(x.basis,x.basis)
X <- eval.basis(t.vec, x.basis)
pen.mat <- getbasispenalty(x.basis,2)
y.true <- 2*t.vec - .1*t.vec^2
noise.sd <- .25*(2 + (1:length(y.true)/20))
y.obs = y.true+rnorm(length(y.true),0,noise.sd)
W = diag(1/noise.sd)
W.fd <- smooth.basis(t.vec,diag(W),x.basis)
pen.mat.W <- inprod(x.basis,x.basis,2,2,wtfd = W.fd$fd * W.fd$fd)
plot(t.vec,y.obs)
points(t.vec,y.true,col="green")
# Chunk 4
tr <- function(X) sum(diag(X))
get_hat <- function(X,w=NULL,lambda=0,pen.mat = diag(1,ncol(X))){
X = as.matrix(X)
if(is.null(w)){
z = X
}
else{
z = w %*% X
}
H = z %*% solve(t(z) %*% z + lambda*pen.mat) %*% t(z)
return(H)
}
get_naive_press <- function(X,y,W=NULL,lambda=0,pen.mat = F){
n = length(y)
output.vec = rep(0,n)
X = as.matrix(X)
if(is.null(W)==F){
X = W %*% X
y = W %*% y
}
for(ii in 1:n){
z = X[-ii,] %>% as.matrix()
temp.y = y[-ii] %>% as.matrix()
temp.beta = solve(t(z) %*% z + lambda*pen.mat) %*% t(z) %*% temp.y
pred = t(as.matrix(X[ii,])) %*% temp.beta
output.vec[ii] = (pred - y[ii])^2
}
return((1/n) * sum(output.vec))
}
get_naive_gcv <- function(X,y,W=NULL,lambda=0,pen.mat = diag(1,ncol(X))){
A = get_hat(X,W,lambda,pen.mat)
n = length(y)
output.vec = rep(0,n)
X = as.matrix(X)
if(is.null(W)==F){
X = W %*% X
y = W %*% y
}
for(ii in 1:n){
z = X[-ii,] %>% as.matrix()
temp.y = y[-ii] %>% as.matrix()
temp.beta = solve(t(z) %*% z + lambda*pen.mat) %*% t(z) %*% temp.y
pred = t(as.matrix(X[ii,])) %*% temp.beta
gcv.w_i = (1-A[ii,ii])/(1-(1/n)*tr(A))
output.vec[ii] = (pred - y[ii])^2 * gcv.w_i
}
return((1/n) * sum(output.vec))
}
get_coefs <- function(X,y, lambda,pen.mat, W = NULL){
# If a W is given, COEFS WILL BE FOR THE SMOOTH IN THE TRANSFORMED SPACE
n = nrow(X)
y = as.matrix(y,length(y),1)
if(is.null(W)==F){
X = W %*% X
y = W %*% y
}
output = solve(t(X) %*% X + pen.mat*lambda) %*% t(X) %*% y
return(output)
}
true.coefs <- get_coefs(X, y.true,0,pen.mat)
inprod.mat <- inprod(x.basis, x.basis)
RMSE <- function(coef1,coef2, inprod.mat = inprod.mat){
r.coef = coef1 - coef2
mse = t(r.coef) %*% inprod.mat %*% (r.coef)
return(sqrt(mse[1,1]))
}
test.smooth <- smooth.basis(t.vec, y = y.obs, fdPar(x.basis,lambda = 1))
test.smooth2 <- smooth.basis(t.vec, y = y.obs, fdPar(x.basis,lambda = .99))
all.equal(get_coefs(X,y.obs,1,pen.mat),test.smooth$fd$coefs)
RMSE(test.smooth$fd$coefs,test.smooth2$fd$coefs, inprod.mat)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,warning=F,message=F)
library(tidyverse)
# Chunk 2
set.seed(11235)
library(fda)
t.vec = seq(.1,9.9,.1)
x.basis <- create.bspline.basis(range(t.vec),nbasis = round(.8*length(t.vec)))
inprod.mat <- inprod(x.basis,x.basis)
X <- eval.basis(t.vec, x.basis)
pen.mat <- getbasispenalty(x.basis,2)
y.true <- 2*t.vec - .1*t.vec^2
noise.sd <- .25*(2 + (1:length(y.true)/20))
y.obs = y.true+rnorm(length(y.true),0,noise.sd)
W = diag(1/noise.sd)
W.fd <- smooth.basis(t.vec,diag(W),x.basis)
pen.mat.W <- inprod(x.basis,x.basis,2,2,wtfd = W.fd$fd * W.fd$fd)
plot(t.vec,y.obs)
points(t.vec,y.true,col="green")
# Chunk 4
tr <- function(X) sum(diag(X))
get_hat <- function(X,w=NULL,lambda=0,pen.mat = diag(1,ncol(X))){
X = as.matrix(X)
if(is.null(w)){
z = X
}
else{
z = w %*% X
}
H = z %*% solve(t(z) %*% z + lambda*pen.mat) %*% t(z)
return(H)
}
get_naive_press <- function(X,y,W=NULL,lambda=0,pen.mat = F){
n = length(y)
output.vec = rep(0,n)
X = as.matrix(X)
if(is.null(W)==F){
X = W %*% X
y = W %*% y
}
for(ii in 1:n){
z = X[-ii,] %>% as.matrix()
temp.y = y[-ii] %>% as.matrix()
temp.beta = solve(t(z) %*% z + lambda*pen.mat) %*% t(z) %*% temp.y
pred = t(as.matrix(X[ii,])) %*% temp.beta
output.vec[ii] = (pred - y[ii])^2
}
return((1/n) * sum(output.vec))
}
get_naive_gcv <- function(X,y,W=NULL,lambda=0,pen.mat = diag(1,ncol(X))){
A = get_hat(X,W,lambda,pen.mat)
n = length(y)
output.vec = rep(0,n)
X = as.matrix(X)
if(is.null(W)==F){
X = W %*% X
y = W %*% y
}
for(ii in 1:n){
z = X[-ii,] %>% as.matrix()
temp.y = y[-ii] %>% as.matrix()
temp.beta = solve(t(z) %*% z + lambda*pen.mat) %*% t(z) %*% temp.y
pred = t(as.matrix(X[ii,])) %*% temp.beta
gcv.w_i = (1-A[ii,ii])/(1-(1/n)*tr(A))
output.vec[ii] = (pred - y[ii])^2 * gcv.w_i
}
return((1/n) * sum(output.vec))
}
get_coefs <- function(X,y, lambda,pen.mat, W = NULL){
# If a W is given, COEFS WILL BE FOR THE SMOOTH IN THE TRANSFORMED SPACE
n = nrow(X)
y = as.matrix(y,length(y),1)
if(is.null(W)==F){
X = W %*% X
y = W %*% y
}
output = solve(t(X) %*% X + pen.mat*lambda) %*% t(X) %*% y
return(output)
}
true.coefs <- get_coefs(X, y.true,0,pen.mat)
inprod.mat <- inprod(x.basis, x.basis)
RMSE <- function(coef1,coef2, inprod.mat = inprod.mat){
r.coef = coef1 - coef2
mse = t(r.coef) %*% inprod.mat %*% (r.coef)
return(sqrt(mse[1,1]))
}
test.smooth <- smooth.basis(t.vec, y = y.obs, fdPar(x.basis,lambda = 1))
test.smooth2 <- smooth.basis(t.vec, y = y.obs, fdPar(x.basis,lambda = .99))
all.equal(get_coefs(X,y.obs,1,pen.mat),test.smooth$fd$coefs)
RMSE(test.smooth$fd$coefs,test.smooth2$fd$coefs, inprod.mat)
l.vec <- exp(seq(1,6,.2))
output <- matrix(NA,40,8)
output.objects <- list()
for(ii in 1:nrow(output)){
noise <- rnorm(length(y.true),0,noise.sd)
y.obs <- y.true + noise
# Do a grid search to find the optimal smoothing parameters using the
# different procedures
unweighted.press.vec <- rep(NA,length(l.vec))
unweighted.gcv.vec <- rep(NA,length(l.vec))
weighted.press.vec <- rep(NA,length(l.vec))
weighted.gcv.vec <- rep(NA,length(l.vec))
for(jj in 1:length(l.vec)){
unweighted.press.vec[jj] <- get_naive_press(X = X, y = y.obs,lambda = l.vec[jj],
pen.mat = pen.mat)
unweighted.gcv.vec[jj] <- get_naive_gcv(X = X, y = y.obs,lambda = l.vec[jj],
pen.mat = pen.mat)
weighted.press.vec[jj] <- get_naive_press(X = X, y = y.obs,lambda = l.vec[jj],
pen.mat = pen.mat.W, W = W)
weighted.gcv.vec[jj] <- get_naive_gcv(X = X, y = y.obs,lambda = l.vec[jj],
pen.mat = pen.mat.W, W = W)
}
up.i = which.min(unweighted.press.vec)
ug.i = which.min(unweighted.gcv.vec)
wp.i = which.min(weighted.press.vec)
wg.i = which.min(weighted.gcv.vec)
up.l = l.vec[up.i]
ug.l = l.vec[ug.i]
wp.l = l.vec[wp.i]
wg.l = l.vec[wg.i]
# Coefs give smooth in original space
up.coefs = get_coefs(X,y.obs,up.l,pen.mat)
ug.coefs = get_coefs(X,y.obs,ug.l,pen.mat)
# Coefs give smooth in transformed space
# i.e., W %*% X %*% wp.coefs gives weighted-PRESS-optimal smooth
# in transformed space
wp.coefs = get_coefs(X,y.obs,wp.l,pen.mat.W,W)
wg.coefs = get_coefs(X,y.obs,wg.l,pen.mat.W,W)
# W %*% X %*% wp.coefs gives you the smoothed predictions in transformed space
# To transform back, multiply by W^{-1}
# The predictions back in the original space are
# W^{-1} %*% W %*% X %*% wp.coefs = X %*% wp.coefs
# Because both the weighted and unweighted coefs are multiplied by (the
# original) X, I can use the RMSE function defined above without adjustment.
up.rmse = RMSE(up.coefs,true.coefs,inprod.mat)
ug.rmse = RMSE(ug.coefs,true.coefs,inprod.mat)
wp.rmse = RMSE(wp.coefs,true.coefs,inprod.mat)
wg.rmse = RMSE(wg.coefs,true.coefs,inprod.mat)
output[ii,] = c(up.l,ug.l,wp.l,wg.l,up.rmse,ug.rmse,wp.rmse,wg.rmse)
output.objects[[ii]] = list(noise,y.obs,
unweighted.press.vec,
unweighted.gcv.vec,
weighted.press.vec,
weighted.gcv.vec,
up.coefs,ug.coefs,wp.coefs,wg.coefs)
}
colnames(output) <- c("up.l","ug.l","wp.l","wg.l","up.rmse","ug.rmse","wp.rmse","wg.rmse")
output <- round(output,3) %>% as.data.frame() %>%
mutate(ID=row_number(),.before = 1,
p.improved = (wp.rmse<up.rmse),
g.improved =(wg.rmse<ug.rmse))
output
mean(output$g.improved)
head(pen.mat)
head(pen.mat.W)
post.w <- readRDS("post.w.rds")
post.b <- readRDS("post.b.rds")
post.h <- readRDS("post.h.rds")
A <- post.w %>% head %>% select(2:4) %>% round(3)
B <- post.b %>% head %>% select(2:4) %>% round(3)
C <- post.h %>% head %>% select(2:4) %>% round(3)
rownames(A) = NULL
rownames(B) = NULL
rownames(C) = NULL
ids <- data.frame(ID = substr(post.w$rowname,31,40)) %>% head
ttt <- cbind(ids,A,B,C)
names(ttt) <- c("ID","W 2.5%","W 50%", "W 97.5%","B 2.5%","B 50%", "B 97.5%","H 2.5%","H 50%", "H 97.5%")
ttt %>% kable(caption="Posterior intervals for several officers for three races. From left to right: white, Black, Hispanic", booktabs=T) %>%
kable_styling(latex_options = c("scale_down"))
library(tidyverse)
library(knitr)
library(kableExtra)
post.w <- readRDS("post.w.rds")
post.b <- readRDS("post.b.rds")
post.h <- readRDS("post.h.rds")
A <- post.w %>% head %>% select(2:4) %>% round(3)
B <- post.b %>% head %>% select(2:4) %>% round(3)
C <- post.h %>% head %>% select(2:4) %>% round(3)
rownames(A) = NULL
rownames(B) = NULL
rownames(C) = NULL
ids <- data.frame(ID = substr(post.w$rowname,31,40)) %>% head
ttt <- cbind(ids,A,B,C)
names(ttt) <- c("ID","W 2.5%","W 50%", "W 97.5%","B 2.5%","B 50%", "B 97.5%","H 2.5%","H 50%", "H 97.5%")
ttt %>% kable(caption="Posterior intervals for several officers for three races. From left to right: white, Black, Hispanic", booktabs=T) %>%
kable_styling(latex_options = c("scale_down"))
post.w <- readRDS("post.w.rds")
post.b <- readRDS("post.b.rds")
post.h <- readRDS("post.h.rds")
A <- post.w %>% head %>% select(2:4) %>% round(3)
B <- post.b %>% head %>% select(2:4) %>% round(3)
C <- post.h %>% head %>% select(2:4) %>% round(3)
rownames(A) = NULL
rownames(B) = NULL
rownames(C) = NULL
ids <- data.frame(ID = substr(post.w$rowname,31,40)) %>% head
ttt <- cbind(ids,A,B,C)
names(ttt) <- c("ID","W 2.5%","W 50%", "W 97.5%","B 2.5%","B 50%", "B 97.5%","H 2.5%","H 50%", "H 97.5%")
ttt %>% kable(caption="Posterior intervals for several officers for three races. From left to right: white, Black, Hispanic", booktabs=T) %>%
kable_styling(latex_options = c("scale_down"))
post.w <- readRDS("post.w.rds")
getwd()
setwd("E:/FunkyStats/")
post.w <- readRDS("post.w.rds")
post.b <- readRDS("post.b.rds")
post.h <- readRDS("post.h.rds")
A <- post.w %>% head %>% select(2:4) %>% round(3)
B <- post.b %>% head %>% select(2:4) %>% round(3)
C <- post.h %>% head %>% select(2:4) %>% round(3)
rownames(A) = NULL
rownames(B) = NULL
rownames(C) = NULL
ids <- data.frame(ID = substr(post.w$rowname,31,40)) %>% head
ttt <- cbind(ids,A,B,C)
names(ttt) <- c("ID","W 2.5%","W 50%", "W 97.5%","B 2.5%","B 50%", "B 97.5%","H 2.5%","H 50%", "H 97.5%")
ttt %>% kable(caption="Posterior intervals for several officers for three races. From left to right: white, Black, Hispanic", booktabs=T) %>%
kable_styling(latex_options = c("scale_down"))
data.rates <- searches %>% group_by(subject_race,officer_id_hash) %>%
summarise(hit_rate=mean(hit,na.rm=T)) %>%
pivot_wider(id_cols=officer_id_hash,
names_from=subject_race, values_from=hit_rate)
search.counts <- searches %>% group_by(officer_id_hash,subject_race) %>%
summarise(count=n()) %>%
pivot_wider(id_cols=officer_id_hash,
names_from=subject_race, values_from=count)
quantile.mat <- full_join(post.w,post.b,by="rowname",suffix = c(".w",".b")) %>% full_join(.,post.h,by="rowname",suffix=c(".b",".h"))
q.id <- quantile.mat[,1] %>% str_sub(start = 31, end = 40)
quantile.mat$officer_id_hash = q.id
comparisons <- full_join(data.rates,quantile.mat %>% select(officer_id_hash,contains("50%")), by = "officer_id_hash") %>%
full_join(.,search.counts, by = "officer_id_hash")
names(comparisons) <- c("ID","obs.w","obs.b","obs.h",
"post.w","post.b","post.h",
"count.w","count.b","count.h")
#comparisons
par(mfrow=c(1,3))
plot(comparisons$obs.w,comparisons$post.w, main = "Post. Medians - white ", xlab = "Observed rate", ylab="Posterior Median")
abline(0,1,lty=2)
plot(comparisons$obs.b,comparisons$post.b, main = "Post. Medians - Black ", xlab = "Observed rate", ylab="Posterior Median")
abline(0,1,lty=2)
plot(comparisons$obs.h,comparisons$post.h, main = "Post. Medians - Hispanic ", xlab = "Observed rate", ylab="Posterior Median")
abline(0,1,lty=2)
# Chunk 1: setup
knitr::opts_chunk$set(echo = F,message = F,warning = F,dev = 'pdf',fig.pos = '!H',cache = T)
# Chunk 2
#rm(list=ls())
library(tidyverse)
library(janitor)
library(knitr)
library(kableExtra)
library(gridExtra)
# Chunk 3
options(xtable.comment = FALSE)
fancy.summarize <- source("fancy_summarize.R")$value
d <- readRDS("austin.rds")
d <- d %>% mutate(vehicle_year = ifelse(vehicle_year<1960,1960,vehicle_year)) %>%
mutate(vehicle_year = ifelse(vehicle_year > 2017,NA, vehicle_year)) %>%
mutate(vehicle_make = fct_lump_n(vehicle_make,n = 25)) %>%
mutate(vehicle_model = fct_lump_n(vehicle_model,n = 250))
d.numeric.all <- d %>% select(subject_age,subject_sex,frisk_performed,
search_conducted, search_person,
search_vehicle)
d.numeric.search <- d %>% filter(search_conducted==T) %>%
select(contraband_found,contraband_drugs,contraband_weapons,frisk_performed)
d.cat <- d %>% select(subject_race,search_basis,reason_for_stop,
vehicle_make,vehicle_model,vehicle_registration_state,
vehicle_year)
summary.stats1 <- d.numeric.all %>% mutate_all(as.numeric) %>%
mutate(subject_sex = subject_sex - 1) %>%  #defaults to 1/2 coding
as.data.frame() %>%
fancy.summarize(.,nmis=T,uniq=T,latex = T)
summary.stats.tables <- apply(d.cat,2,tabyl)
# Chunk 5
rr <- summary.stats.tables[[1]] %>% select(c(1,2,4))
names(rr) <- c("Race","n","percent")
kable(rr, caption = "Subject race.") %>%
kable_styling(position = "center")
# Chunk 6
rr <- summary.stats.tables[[2]] %>% select(c(1,2,4))
names(rr) <- c("Search basis","n","percent")
kable(rr,caption ="Search basis.") %>%
kable_styling(position = "center")
# Chunk 7
officer.counts <- d %>% group_by(officer_id_hash) %>% summarise(`Number of stops per officer` = n())
boxplot(officer.counts$`Number of stops per officer`)
# Chunk 9
d2 <- d %>% filter(subject_race %in% c("white","black","hispanic")) %>%
mutate(subject_race = fct_lump_min(subject_race,20000))
searches.all <- d2 %>% filter(frisk_performed==T) %>%
tabyl(officer_id_hash)
#quantile(searches.all$n,seq(0,1,.1))
ids.to.keep <- searches.all %>% filter(n>18) %>% pull(officer_id_hash)
searches <- d2 %>% filter(officer_id_hash %in% ids.to.keep) %>%
filter(subject_race %in% c("black","hispanic","white")) %>%
mutate(subject_race = relevel(subject_race,ref="white")) %>%
mutate(subject_race = fct_drop(subject_race)) %>%
filter(frisk_performed==T) %>%
mutate(hit = (contraband_found | contraband_drugs | contraband_weapons)) %>%
select(officer_id_hash, contains("subject_"),
reason_for_stop,frisk_performed,search_conducted,hit,contains("contraband"))
# Chunk 10
hit_rates <- searches %>%
group_by(officer_id_hash,subject_race) %>%
summarise(
hit_rate = mean(contraband_found, na.rm = T)
)
hit_rates <- hit_rates %>%
filter(subject_race %in% c("black", "white", "hispanic")) %>%
spread(subject_race, hit_rate, fill = 0) %>%
rename(white_hit_rate = white) %>%
gather(minority_race, minority_hit_rate, c(black, hispanic)) %>%
arrange(officer_id_hash)
# We'll use this just to make our axes' limits nice and even
max_hit_rate <- hit_rates %>% ungroup %>%
select(ends_with("hit_rate")) %>%
max()
hit_rates %>%
ggplot(aes(
x = white_hit_rate,
y = minority_hit_rate
)) +
geom_point() +
# This sets a diagonal reference line (line of equal hit rates)
geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
# These next few lines just make the axes pretty and even
scale_x_continuous("White hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
scale_y_continuous("Minority hit rate",
limits = c(0, max_hit_rate + 0.01),
labels = scales::percent
) +
# This makes sure that 1% on the x-axis is the same as 1% on the y-axis
coord_fixed() +
# This allows us to compare black v. white and Hispanic v. white side by
# side, in panels
facet_grid(. ~ minority_race)
# Depending on your version of ggplot2, you may be able to use the syntax
# below (the newer ggplot2 syntax)---which is clearer, in my opinion.
# But older versions of ggplot2 will only accept the above syntax
# facet_grid(cols = vars(minority_race))
# Chunk 11
hit_rates_binary <- searches %>%
filter(subject_race %in% c("black","hispanic","white")) %>%
mutate(subject_race = fct_drop(subject_race)) %>%
mutate(subject_race = relevel(subject_race,ref="white")) %>%
group_by(officer_id_hash,subject_race) %>%
summarise(
hit_rate = mean(contraband_found, na.rm = T),
nsearches = n(),
nhits = sum(contraband_found,na.rm=T)
)
# Chunk 12
library(rstanarm)
summary_stats <- function(posterior) {
x <- invlogit(posterior)  # log-odds -> probabilities
t(apply(x, 2, quantile, probs = c(0.025, 0.5, 0.975)))
}
shift_draws <- function(draws) {
sweep(draws[, -1], MARGIN = 1, STATS = draws[, 1], FUN = "+")
}
# Chunk 14
post.w <- readRDS("post.w.rds")
post.b <- readRDS("post.b.rds")
post.h <- readRDS("post.h.rds")
A <- post.w %>% head %>% select(2:4) %>% round(3)
B <- post.b %>% head %>% select(2:4) %>% round(3)
C <- post.h %>% head %>% select(2:4) %>% round(3)
rownames(A) = NULL
rownames(B) = NULL
rownames(C) = NULL
ids <- data.frame(ID = substr(post.w$rowname,31,40)) %>% head
ttt <- cbind(ids,A,B,C)
names(ttt) <- c("ID","W 2.5%","W 50%", "W 97.5%","B 2.5%","B 50%", "B 97.5%","H 2.5%","H 50%", "H 97.5%")
ttt %>% kable(caption="Posterior intervals for several officers for three races. From left to right: white, Black, Hispanic", booktabs=T) %>%
kable_styling(latex_options = c("scale_down"))
# Chunk 15
data.rates <- searches %>% group_by(subject_race,officer_id_hash) %>%
summarise(hit_rate=mean(hit,na.rm=T)) %>%
pivot_wider(id_cols=officer_id_hash,
names_from=subject_race, values_from=hit_rate)
search.counts <- searches %>% group_by(officer_id_hash,subject_race) %>%
summarise(count=n()) %>%
pivot_wider(id_cols=officer_id_hash,
names_from=subject_race, values_from=count)
quantile.mat <- full_join(post.w,post.b,by="rowname",suffix = c(".w",".b")) %>% full_join(.,post.h,by="rowname",suffix=c(".b",".h"))
q.id <- quantile.mat[,1] %>% str_sub(start = 31, end = 40)
quantile.mat$officer_id_hash = q.id
comparisons <- full_join(data.rates,quantile.mat %>% select(officer_id_hash,contains("50%")), by = "officer_id_hash") %>%
full_join(.,search.counts, by = "officer_id_hash")
names(comparisons) <- c("ID","obs.w","obs.b","obs.h",
"post.w","post.b","post.h",
"count.w","count.b","count.h")
#comparisons
par(mfrow=c(1,3))
plot(comparisons$obs.w,comparisons$post.w, main = "Post. Medians - white ", xlab = "Observed rate", ylab="Posterior Median")
abline(0,1,lty=2)
plot(comparisons$obs.b,comparisons$post.b, main = "Post. Medians - Black ", xlab = "Observed rate", ylab="Posterior Median")
abline(0,1,lty=2)
plot(comparisons$obs.h,comparisons$post.h, main = "Post. Medians - Hispanic ", xlab = "Observed rate", ylab="Posterior Median")
abline(0,1,lty=2)
# Chunk 16
#ids.to.keep <- stanfit.h$data$officer_id_hash
output = rep(0,nrow(comparisons))
median.mat <- quantile.mat %>% select(contains("50%")) %>% as.matrix()
for (ii in 1:nrow(comparisons)){
current.row <- comparisons[ii,5:7]
temp = rowMeans(current.row,na.rm=T)
ssr = sum((current.row - temp)^2, na.rm = T)
output[ii] = ssr
}
fairness.df <- data.frame(ID = comparisons$ID, score = output)
fairness.df %>% ggplot(aes(x=score)) + geom_histogram(bins=100)
ex5 <- fairness.df %>% arrange(desc(score)) %>% pull(ID)
ex5 <- ex5[1:5]
comparisons %>% filter(ID %in% ex5) %>% mutate_if(is.numeric,round,3) %>%
kable(caption="Highest 5 scores.", booktabs=T) %>%
kable_styling(latex_options = "scale_down")
log(.5)
exp(.5)
